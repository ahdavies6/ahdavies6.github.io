I study [large, pretrained neural language models (LLMs)](https://arxiv.org/abs/2303.18223) like ChatGPT -- specifically, *how they learn and represent the meaning in language.*
Deep neural networks in general, and LLMs in particular, are usually seen as being "black boxes": we do not (fully) understand what goes on inside them, only how they learn and how well they perform on the benchmarks we create for them (e.g., [SuperGLUE](https://super.gluebenchmark.com/), [LAMA](https://github.com/facebookresearch/LAMA), [BIG-bench](https://github.com/google/BIG-bench), etc.).
These benchmarks can be valuable tools for evaluating progress in natural language processing, but they are characteristically agnostic to *how*&nbsp; LLMs achieve the performance they do, making them [imperfect measures](https://arxiv.org/abs/2111.15366) of progress toward general language understanding.
For example, current LLMs are already able to achieve super-human performance on many [popular](https://gluebenchmark.com/leaderboard) [language](https://super.gluebenchmark.com/leaderboard) [benchmarks](https://rajpurkar.github.io/SQuAD-explorer/).
As state-of-the-art LLMs continue to achieve or outperform human-level performance on such benchmarks, it is natural to ask whether they truly understand language at the level of fluent human speakers, or whether "black box" evaluation approaches (where only inputs and outputs are considered, and internal mechanisms are ignored) are simply insufficient to adequately measure progress toward [human-level linguistic competence](https://arxiv.org/abs/2301.06627).

My goal is to develop research approaches that can help us answer questions like these.
In doing so, I believe that it is necessary to study LLMs' [internal representation of linguistic structure](https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and), and [how this representation is used](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091/Amnesic-Probing-Behavioral-Explanation-with) as they perform natural language tasks.
I am currently researching the structure of LLMs' representations of lexical semantics via [competence-based analysis of language models (CALM)](https://arxiv.org/abs/2303.00333) to study what elements of meaning are acquired and used by LLMs, [multimodal vision-and-language models](https://arxiv.org/abs/2306.13549), and other varieties of [foundation models](https://arxiv.org/abs/2108.07258).
A better understanding of what is actually learned by these models (and what is not) will be essential to predict and explain their [current limitations](https://arxiv.org/abs/2301.06627), where they [may (or may not) be safely applied](https://arxiv.org/pdf/2111.15366.pdf#page=10), and develop the next generation of more human-like, generalizable foundation models.
