
I study [neural language models](https://towardsdatascience.com/neural-language-models-32bec14d01dc), primarily *the extent to which they process language like humans do*.
Deep neural networks in general, and neural language models in particular, are usually seen as being "black boxes": we do not (fully) understand what goes on inside them, only [how they learn](https://www.youtube.com/watch?v=IHZwWFHWa-w) and how well they perform on the benchmarks we create for them (e.g. [SuperGLUE](https://super.gluebenchmark.com/)).
These benchmarks can be valuable tools for evaluating progress in language modeling (and natural language processing more generally), but they are (usually) agnostic to *how*&nbsp; these models achieve the performance they do.
How can we tell *what they have actually learned*&nbsp; about language (and whether they reflect aspects of human language acquisition and processing)?

To better understand and explain the inner workings of these models, I research [machine learning interpretability](https://dl.acm.org/doi/pdf/10.1145/3236386.3241340).
I am most interested in *how these models represent word meaning*, which involves [computational semantics](https://en.wikipedia.org/wiki/Computational_semantics) and [representation learning](https://analyticsindiamag.com/a-comprehensive-guide-to-representation-learning-for-beginners/).
