I study [neural language models (LMs)](https://arxiv.org/pdf/1906.03591.pdf), specifically, *how they learn and represent the meaning in language*.
Deep neural networks in general, and neural LMs in particular, are usually seen as being "black boxes": we do not (fully) understand what goes on inside them, only how they learn and how well they perform on the benchmarks we create for them (e.g., [SuperGLUE](https://super.gluebenchmark.com/), [LAMA](https://github.com/facebookresearch/LAMA), [BIG-bench](https://github.com/google/BIG-bench), etc.).
These benchmarks can be valuable tools for evaluating progress in natural language processing, but they are characteristically agnostic to *how*&nbsp; LMs achieve the performance they do,
making them [imperfect measures](https://arxiv.org/pdf/2111.15366.pdf) of progress toward general language understanding.
<!-- How can we tell *what LMs actually learn*&nbsp; about language, (and whether they reflect aspects of human language acquisition and processing)? -->
<!-- Do they ["understand" the meanings of words, phrases, and sentences](https://aclanthology.org/2020.acl-main.463.pdf)? -->
<!-- Or are they just impressive [parrots](https://dl.acm.org/doi/10.1145/3442188.3445922)? -->

<!-- Humans learn language through richly [multimodal, embodied interaction](https://doi.org/10.1016/j.cognition.2012.06.016) with the world and [other linguistic agents](https://doi.org/10.1111/j.1467-7687.2005.00445.x).
Is this simply a contingent feature of human language acquisition, or a fundamental requirement for fully understanding language? Do [grounded language models](https://aclanthology.org/2020.emnlp-main.703.pdf) "know" something about language that [cannot be learned](https://aclanthology.org/2020.acl-main.463.pdf) [from text alone](https://arxiv.org/pdf/2008.01766.pdf)? -->
For example, while humans ground language learning in richly [multimodal, embodied interaction](https://doi.org/10.1016/j.cognition.2012.06.016) with the world and [other linguistic agents](https://doi.org/10.1111/j.1467-7687.2005.00445.x), current LMs trained only on text data are already able to achieve super-human performance on many [popular](https://gluebenchmark.com/leaderboard) [language](https://super.gluebenchmark.com/leaderboard) [benchmarks](https://rajpurkar.github.io/SQuAD-explorer/).
Should continued progress on such benchmarks be interpreted as a sign that grounding is simply a contingent feature of human language acquisition, and that text-only language modeling is enough for human-like language understanding?
<!-- Is this a sign that grounding is simply a contingent feature of human language acquisition, and that current LMs possess genuine language understanding -->
Or is this [only possible](https://aclanthology.org/2020.acl-main.463.pdf) for [grounded language models](https://aclanthology.org/2020.emnlp-main.703.pdf)?

My goal is to develop research approaches that can help us answer questions like these.
In particular, I want to investigate whether there are elements of semantic representations learned by multimodally grounded LMs (e.g., [CLIP](https://arxiv.org/pdf/2103.00020.pdf), [ALIGN](https://arxiv.org/pdf/2102.05918.pdf), [MERLOT](https://arxiv.org/pdf/2106.02636.pdf) ([Reserve](https://arxiv.org/pdf/2201.02639.pdf)), etc.) that are not acquired by text-only models.
I am currently researching how [counterfactual methodologies](https://christophm.github.io/interpretable-ml-book/counterfactual.html#counterfactual) can be applied to [locate](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091/Amnesic-Probing-Behavioral-Explanation-with) and [manipulate](https://arxiv.org/pdf/2202.05262.pdf) semantic content acquired by such models, and test whether they have indeed captured elements of linguistic meaning that leading text-only models have not.
