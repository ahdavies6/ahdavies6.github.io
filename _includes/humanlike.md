
As humans, we are primed to [perceive non-human entities as having humanlike properties](https://en.wikipedia.org/wiki/Anthropomorphism), especially when they exhibit some degree of intelligent behavior. Have you ever described a computer stuck on a loading screen as "thinking", or a plant growing toward the sun as "wanting" light?
It can be especially difficult not to anthropomorphize systems which are capable of generating natural language (a pattern which has been [evident since the earliest of chatbots](https://steemit.com/science/@etherealcreation/eliza-beginning-of-era-of-artificial-intelligence)).
Given this tendency, how should we read the success of cutting-edge neural language models like [GPT-3](https://arxiv.org/abs/2005.14165)?
Have they actually learned to process language in ways that are at all comparable to how we do?
Do they "know" the meanings of words, phrases, and sentences?
Or are they just impressive [parrots](https://dl.acm.org/doi/10.1145/3442188.3445922)?
How would we even know?
(Here's my [favorite discussion on this topic](https://aclanthology.org/2020.acl-main.463.pdf).)
My goal is to develop research approaches that can help us answer questions like these.
